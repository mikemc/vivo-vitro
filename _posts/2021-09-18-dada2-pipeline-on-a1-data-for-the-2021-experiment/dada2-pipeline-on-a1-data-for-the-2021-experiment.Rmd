---
title: "DADA2 pipeline on the A1 data for the 2021 experiment"
description: |
  This document analyzes the amplicon data from the A1 sequencing center for the 2021 experiment following the workflow used for the 2019 experiment.
author:
  - name: Michael R. McLaren
categories:
  - 2021 experiment
  - bioinformatics
date: "2021-09-18"
output:
  distill::distill_article:
    self_contained: false
    toc: true
---

```{r, include = FALSE}
# knitr chunk options
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  cache = FALSE,
  include = TRUE, echo = TRUE,
  warning = TRUE, message = TRUE, 
  fig.width = 7, fig.height = 7
)
```

This R-markdown document analyzes the amplicon data from the A1 sequencing center for the 2021 experiment, following the workflow in 'analysis/2020-11-01-dada2/a1-dada2.html' that was used for the 2019 experiment.

## Setup

Load non-bioinformatic libraries,

```{r}
library(here)
library(tidyverse)
library(fs)
```

Next, I will install and load the previous DADA2 version that I used for the 2020-11-01 analysis of the 2019 data: version 1.18.0 from Bioconductor, which corresponds to the v1.18 tag (a20a676) on GitHub (https://github.com/benjjneb/dada2/releases/tag/v1.18).
(Only run if not previously done.)

```{r}
dr <- here('lib', 'R', 'library-for-dada2-1.18.0')
if (!dir_exists(dr)) 
  dir_create(dr)
devtools::dev_mode(TRUE, dr)
if (packageVersion("dada2") != "1.18.0")
  remotes::install_github('benjjneb/dada2@v1.18')
```

Load DADA2 and functions from bioinformatics libraries

```{r}
stopifnot(packageVersion("dada2") == "1.18.0")
library(dada2)

import::from(Biostrings, complement, reverseComplement, width, DNAString,
  readDNAStringSet)
import::from(DECIPHER, TrimDNA)
```

Path for saving dada2 results:

```{r}
results_path <- here("output", "2021", "a1", "dada2")
dir.create(results_path, recursive = TRUE)
```

Path to the fastq files:

```{r path}
reads_path <- here("data", "2021", "a1", "reads")
dir_ls(path(reads_path, "raw")) %>% path_file %>% head
x <- dir_ls(path(reads_path, "raw")) %>% path_file
```

Let's get a data frame with the sample names, paths to the raw reads, and paths to the (to-be-created) filtered reads.
Note the creation of DNA sample names from the sequence files: I revert the hyphens back to underscores, make the names of the A1 control samples lowercase, and prepend 2021 to ensure uniqueness and distinguishability from the 2019 DNA samples.

```{r}
ftb <- tibble(path = dir_ls(path(reads_path, "raw"), glob = "*.fastq.gz")) %>%
  mutate(
    read_direction = case_when(
      str_detect(path, "_R1_") ~ "R1",
      str_detect(path, "_R2_") ~ "R2"
    ),
    fastq_sample_id = str_extract(path %>% path_file, "[^_]+"),
    dna_sample_id = case_when(
      fastq_sample_id %in% c('MockZymoPos', 'WaterNeg') ~
        str_to_lower(fastq_sample_id),
      TRUE ~ str_replace_all(fastq_sample_id, "-", "_"),
      ),
    dna_sample_id = str_c("2021_", dna_sample_id),
    sample_id = str_c("A1_", dna_sample_id),
    path_filt = path(reads_path, "filtered", 
      str_glue("{fastq_sample_id}_{read_direction}_filt.fastq.gz")
    )
  ) %>%
  pivot_wider(names_from = read_direction, values_from = c(path, path_filt))
# Check
ftb %>% head %>% mutate(across(starts_with("path"), path_file)) %>% glimpse
```

## Check sequences

The primers, from Kozich et al, are as previously,

```{r}
primers <- c(
  R1 = "GTGCCAGCMGCCGCGGTAA",
  R2 = "TAATCTWTGGGVHCATCAGG"
)
```

The corresponding amplicon sequences in our reference 16S sequences (excluding the primer region) are

```{r}
dna <- Biostrings::readDNAStringSet(
  here("output", "strain-data", "reference-16s-genes.fasta")
)
primers.dna <- primers %>% map(DNAString)
x <- TrimDNA(dna, primers.dna[[1]], primers.dna[[2]] %>% complement, 
  type = "sequences")
width(x) %>% summary
width(x) %>% table
```

The length of the amplicon is 253-254 bp _excluding_ the primer sequences.

Next, let's inspect the sequences for one sample and confirm that the primer sequences ar absent,

```{r}
reads <- ftb %>% 
  slice(1)  %>% 
  select(R1 = path_R1, R2 = path_R2) %>%
  c %>%
  map(readDNAStringSet, format = "fastq")
reads %>% map(head, 2)
x %>% head(2)
```

As expected for this protocol, the primer sequences are not present in the reads and so do not need to be trimmed.

## Pick truncation params

Last time, I determined that the length of the target region (excluding primer sequences) in our reference genomes was 253-254 bp, and that aiming for an overlap of 30bp for a 260bp amplicon was a safe choice.
For the 2019 data, we used the following truncation parameters, based on this choice and the observed quality profiles.

```{r}
trim_left <- c(R1 = 0, R2 = 0)
target_len <- 260
trunc_len <- c(R1 = 145, R2 = 145)
# should overlap by at least
sum(trunc_len) - sum(trim_left) - target_len
```

Let's check some quality profiles to ensure these truncation lengths still look good,

```{r}
set.seed(42)
idx <- sample(nrow(ftb), 9)
idx
ftb %>% slice(idx) %>% pull(sample_id)
qps <- ftb %>%
  slice(idx) %>%
  select(R1 = path_R1, R2 = path_R2) %>%
  c %>%
  map(plotQualityProfile)
```

```{r}
p1 <- qps[["R1"]] +
  geom_hline(yintercept = 30, color = "grey") +
  geom_vline(xintercept = trunc_len[1], color = "darkred") +
  ggtitle("Forward reads")
p2 <- qps[["R2"]] +
  geom_hline(yintercept = 30, color = "grey") +
  geom_vline(xintercept = trunc_len[2], color = "darkred") +
  ggtitle("Reverse reads")
```

```{r, dim = c(10, 10)}
p1
```

```{r, dim = c(10, 10)}
p2
```

```{r}
ggsave(plot = p1, file.path(results_path, "quality-profiles-r1.pdf"),
    width = 10, height = 10, units = "in")
ggsave(plot = p2, file.path(results_path, "quality-profiles-r2.pdf"),
    width = 10, height = 10, units = "in")
```

The sequencing quality looks excellent, and these truncation parameters will work fine.

## Filter and trim

First, delete the filtered reads from any previous runs (if they exist),

```{r}
if (dir_exists(path(reads_path, "filtered")))
  dir_delete(path(reads_path, "filtered"))
```

and set up lists with the paths to the forward and reverse reads (raw and filtered). Note, the file paths are named by sample id so that later DADA2 functions will name the samples by sample id instead of file name.

```{r}
path_raw <- list(R1 = ftb$path_R1, R2 = ftb$path_R2) %>%
  map(set_names, ftb$sample_id)
path_filt <- list(R1 = ftb$path_filt_R1, R2 = ftb$path_filt_R2) %>%
  map(set_names, ftb$sample_id)
# Check
path_filt %>% map(tail, 4) %>% map(names)
path_filt %>% map(tail, 4) %>% map(path_file)
```

Then filter and trim.

```{r}
max_ee <- c(R1 = 2, R2 = 2)
out <- filterAndTrim(
  path_raw[[1]], path_filt[[1]],
  path_raw[[2]], path_filt[[2]],
  trimLeft = trim_left, truncLen = trunc_len, 
  maxN = 0, maxEE = max_ee, truncQ = 2, rm.phix = TRUE, 
  compress = TRUE, multithread = TRUE)
head(out)
# Save in case of crash downstream
saveRDS(out, file.path(results_path, "filt-out.Rds"))
```

## Learn the error rates

```{r}
errs <- path_filt %>%
  map(learnErrors, multithread = TRUE)
saveRDS(errs, file.path(results_path, "error-models.Rds"))
```

```{r}
plotErrors(errs[[1]], nominalQ = TRUE)
ggsave(file.path(results_path, "error-profile-r1.pdf"),
    width = 9, height = 9, units = "in")
plotErrors(errs[[2]], nominalQ = TRUE)
ggsave(file.path(results_path, "error-profile-r2.pdf"),
    width = 9, height = 9, units = "in")
```

## Sample inference

The following settings for the `dada()` inference step are used:

- Pseudo-pooling used to increase sensitivity
  - `pool = "pseudo"` option 
  - `PSEUDO_PREVALENCE = 4` (increased from default of 2 to reduce the number
    of spurious low-prev ASVs seen in the pilot)
- The inference step (`dada()`) is run two different ways: The default settings, and with `OMEGA_C = 2` (which prevents error correction, so that we can assign abundances using just the error-free read-pairs).

### Default `OMEGA_C`

Sample inference w/ `dada()`

```{r dada}
dadas <- map2(
  path_filt,
  errs,
  dada,
  multithread = TRUE, pool = "pseudo",
  PSEUDO_PREVALENCE = 4
)
saveRDS(dadas, file.path(results_path, "dadas-1.Rds"))
```

Check the results:

```{r see-dada}
dadas[[1]][[1]]
head(getSequences(dadas[[1]][[1]]), 2)
```

Merge paired reads,

```{r merge, message=FALSE}
mergers <- mergePairs(
  dadas[[1]], path_filt[[1]], 
  dadas[[2]], path_filt[[2]],
  verbose = TRUE)
saveRDS(mergers, file.path(results_path, "mergers-1.Rds"))
```

Construct sequence table

```{r seqtab}
seqtab <- makeSequenceTable(mergers)
saveRDS(seqtab, file.path(results_path, "seqtab-1.Rds"))
dim(seqtab)
```

Inspect distribution of sequence lengths

```{r seqlens}
table(nchar(getSequences(seqtab)))
```

Remove chimeras (with 'pooled' method)

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method = "pooled",
    multithread = TRUE, verbose = TRUE)
saveRDS(seqtab.nochim, file.path(results_path, "seqtab-nochim-1.Rds"))
dim(seqtab.nochim)
sum(seqtab.nochim) / sum(seqtab)
```

Track reads through the pipeline,

```{r track}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadas[[1]], getN), sapply(dadas[[2]], getN), 
    sapply(mergers, getN), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoised_r1", "denoised_r2", "merged",
    "nochim")
rownames(track) <- ftb$sample_id
track <- as_tibble(track, rownames = "sample_id")
write_csv(track, file.path(results_path, "track-1.csv"))
head(track)
```

### `OMEGA_C = 2`

```{r}
rm(dadas, mergers, seqtab, seqtab.nochim, track)
```

Sample inference w/ `dada()`

```{r dada-2}
dadas <- map2(
  path_filt,
  errs,
  dada,
  multithread = TRUE, pool = "pseudo",
  PSEUDO_PREVALENCE = 4,
  OMEGA_C = 2
)
saveRDS(dadas, file.path(results_path, "dadas-2.Rds"))
```

Check the results:

```{r see-dada-2}
dadas[[1]][[1]]
head(getSequences(dadas[[1]][[1]]), 2)
```

Merge paired reads,

```{r merge-2, message=FALSE}
mergers <- mergePairs(
  dadas[[1]], path_filt[[1]], 
  dadas[[2]], path_filt[[2]],
  verbose = TRUE)
saveRDS(mergers, file.path(results_path, "mergers-2.Rds"))
```

Construct sequence table

```{r seqtab-2}
seqtab <- makeSequenceTable(mergers)
saveRDS(seqtab, file.path(results_path, "seqtab-2.Rds"))
dim(seqtab)
```

Inspect distribution of sequence lengths

```{r seqlens-2}
table(nchar(getSequences(seqtab)))
```

Remove chimeras (with 'pooled' method)

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method = "pooled",
    multithread = TRUE, verbose = TRUE)
saveRDS(seqtab.nochim, file.path(results_path, "seqtab-nochim-2.Rds"))
dim(seqtab.nochim)
sum(seqtab.nochim) / sum(seqtab)
```

Track reads through the pipeline,

```{r track-2}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadas[[1]], getN), sapply(dadas[[2]], getN), 
    sapply(mergers, getN), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoised_r1", "denoised_r2", "merged",
    "nochim")
rownames(track) <- ftb$sample_id
track <- as_tibble(track, rownames = "sample_id")
write_csv(track, file.path(results_path, "track-2.csv"))
head(track)
```

## Session info

```{r}
sessioninfo::session_info()
```
